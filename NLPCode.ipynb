{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:843: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "import sys\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "import httplib2\n",
    "import datetime\n",
    "import string\n",
    "import csv\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import textmining\n",
    "\n",
    "\n",
    "# Volatility Numbers Website\n",
    "# http://www.investing.com/indices/volatility-s-p-500-historical-data\n",
    "\n",
    "# Run this if the Directory is not in the Thesis Folder\n",
    "# os.chdir(\"Desktop\\Thesis\")\n",
    "# os.getcwd()\n",
    "# os.chdir(\"Test\")\n",
    "# os.getcwd()\n",
    "\n",
    "# Run this to get Directory in Test Folder\n",
    "# os.chdir(\"Desktop\\Thesis\\Test\")\n",
    "# os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = datetime.datetime.now()\n",
    "date = str(i.month) + \".\" + str(i.day) + \".\" + str(i.year)\n",
    "\n",
    "#Input Article URL\n",
    "\n",
    "url_pol = 'http://www.nbcnews.com/politics'\n",
    "url_mark = 'http://www.nbcnews.com/business/markets'\n",
    "url_perfin = 'http://www.nbcnews.com/business/personal-finance'\n",
    "url_tech = 'http://www.nbcnews.com/tech'\n",
    "\n",
    "articles_pol = []\n",
    "articles_mark = []\n",
    "articles_tech = []\n",
    "articles_perfin = []\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "### Finds all Articles on specific URL\n",
    "### Outputs Article Links to variable (Article)\n",
    "### Prints Articles to .txt file in dir() folder\n",
    "\n",
    "## Politics News\n",
    "# Use Beautiful Soup to analyze source code\n",
    "html_pol = urlopen(url_pol)\n",
    "soup_pol = BeautifulSoup(html_pol, \"lxml\")\n",
    "pol = []\n",
    "\n",
    "#Take artricle link and append to a list\n",
    "for link in soup_pol.findAll('div'):\n",
    "    if 'story-link' in str(link.get('class')):\n",
    "        for a in link:\n",
    "            u = str(link.a.get('href'))\n",
    "            if u not in pol:\n",
    "                pol.append(u)\n",
    "# print(pol)\n",
    "pol_New = []\n",
    "\n",
    "#Creates final article list of URLs\n",
    "for i in pol:\n",
    "    #Restriction: 1) 'http://' has to be in the article title 2) no print if 'http://' already included\n",
    "    if 'http://' not in i and 'video' not in i:\n",
    "        new = \"http://www.nbcnews.com\" + i\n",
    "        pol_New.append(new)   \n",
    "final_pol = pol_New[0:5]\n",
    "# print(final_pol)\n",
    "\n",
    "#For each article, prints all text paragraphs to a .txt file\n",
    "#Tokenizes each article for Date, Pol, Tokenized Word, .txt\n",
    "for i in final_pol:\n",
    "    htmler_pol = urlopen(i)\n",
    "    souper_pol = BeautifulSoup(htmler_pol, \"lxml\")\n",
    "    file = souper_pol.find('title').get_text()[0:20]\n",
    "    words = word_tokenize(file)\n",
    "    stop_title = str(date + \" Pol \" + words[0] + \".txt\")\n",
    "#     print(stop_title)\n",
    "    f_pol = open(stop_title, 'w')\n",
    "    for node in souper_pol.findAll('p'):\n",
    "        f_pol = open(stop_title, 'a')\n",
    "        out = str(''.join(node.findAll(text=True)))\n",
    "        if str(\"Live:\") or str(\"live:\") not in out:\n",
    "            f_pol.write(out)\n",
    "        f_pol.write(\"\\n\")\n",
    "    f_pol.close()\n",
    "\n",
    "    \n",
    "## Market News\n",
    "# Use Beautiful Soup to analyze source code\n",
    "html_mark = urlopen(url_mark)\n",
    "soup_mark = BeautifulSoup(html_mark, \"lxml\")\n",
    "mark = []\n",
    "\n",
    "#Take artricle link and append to a list\n",
    "for link in soup_mark.findAll('div'):\n",
    "    if 'story-link' in str(link.get('class')):\n",
    "        for a in link:\n",
    "            #Restrictions: 1) No repeats 2) Has to be news article 3) Not empty\n",
    "            u = str(link.a.get('href')) \n",
    "            if u not in mark:\n",
    "                mark.append(u)\n",
    "# print(mark)\n",
    "mark_New = []\n",
    "\n",
    "#Creates final article list of URLs\n",
    "for i in mark:\n",
    "    #Restriction: 1) 'http://' has to be in the article title 2) no print if 'http://' already included\n",
    "    if 'http://' not in i and 'video' not in i:\n",
    "        new = \"http://www.nbcnews.com\" + i\n",
    "        mark_New.append(new)   \n",
    "final_mark = mark_New[0:5]\n",
    "# print(final_mark)\n",
    "\n",
    "\n",
    "# For each article, prints all text paragraphs to a .txt file\n",
    "# Tokenizes each article for Date, Mark, Tokenized Word, .txt\n",
    "for i in final_mark:\n",
    "    htmler_mark = urlopen(i)\n",
    "    souper_mark = BeautifulSoup(htmler_mark, \"lxml\")\n",
    "    file = souper_mark.find('title').get_text()[0:20]\n",
    "    words = word_tokenize(file)\n",
    "    stop_title = str(date + \" Mark \" + words[0] + \".txt\")\n",
    "#     print(stop_title)\n",
    "    f_mark = open(stop_title, 'w')\n",
    "    for node in souper_mark.findAll('p'):\n",
    "        f_mark = open(stop_title, 'a')\n",
    "        out = str(''.join(node.findAll(text=True)))\n",
    "        if str(\"Live:\") or str(\"live:\") not in out:\n",
    "            f_mark.write(out)\n",
    "        f_mark.write(\"\\n\")\n",
    "    f_mark.close()\n",
    "\n",
    "    \n",
    "## Technology News\n",
    "# Use Beautiful Soup to analyze source code\n",
    "html_tech = urlopen(url_tech)\n",
    "soup_tech = BeautifulSoup(html_tech, \"lxml\")\n",
    "tech = []\n",
    "\n",
    "#Take artricle link and append to a list\n",
    "for link in soup_tech.findAll('div'):\n",
    "    if 'story-link' in str(link.get('class')):\n",
    "        for a in link:\n",
    "            u = str(link.a.get('href'))\n",
    "            if u not in tech:\n",
    "                tech.append(u)\n",
    "# print(tech)\n",
    "tech_New = []\n",
    "\n",
    "#Creates final article list of URLs\n",
    "for i in tech:\n",
    "    #Restriction: 1) 'http://' has to be in the article title 2) no print if 'http://' already included\n",
    "    if 'http://' not in i and 'video' not in i:\n",
    "        new = \"http://www.nbcnews.com\" + i\n",
    "        tech_New.append(new)   \n",
    "final_tech = tech_New[0:5]\n",
    "# print(final_tech)\n",
    "\n",
    "\n",
    "\n",
    "# For each article, prints all text paragraphs to a .txt file\n",
    "# Tokenizes each article for Date, Tech, Tokenized Word, .txt\n",
    "for i in final_tech:\n",
    "    htmler_tech = urlopen(i)\n",
    "    souper_tech = BeautifulSoup(htmler_tech, \"lxml\")\n",
    "    file = souper_tech.find('title').get_text()[0:20]\n",
    "    words = word_tokenize(file)\n",
    "    stop_title = str(date + \" Tech \" + words[0] + \".txt\")\n",
    "#     print(stop_title)\n",
    "    f_tech = open(stop_title, 'w')\n",
    "    for node in souper_tech.findAll('p'):\n",
    "        f_tech = open(stop_title, 'a')\n",
    "        out = str(''.join(node.findAll(text=True)))\n",
    "        if str(\"Live:\") or str(\"live:\") not in out:\n",
    "            f_tech.write(out)\n",
    "        f_tech.write(\"\\n\")\n",
    "    f_tech.close()\n",
    "    \n",
    "    \n",
    "## Personal Finance News\n",
    "# Use Beautiful Soup to analyze source code\n",
    "html_perfin = urlopen(url_perfin)\n",
    "soup_perfin = BeautifulSoup(html_perfin, \"lxml\")\n",
    "perfin = []\n",
    "\n",
    "#Take artricle link and append to a list\n",
    "for link in soup_perfin.findAll('div'):\n",
    "    if 'story-link' in str(link.get('class')):\n",
    "        for a in link:\n",
    "            u = str(link.a.get('href'))\n",
    "            if u not in perfin:\n",
    "                perfin.append(u)\n",
    "# print(perfin)\n",
    "perfin_New = []\n",
    "\n",
    "#Creates final article list of URLs\n",
    "for i in perfin:\n",
    "    #Restriction: 1) 'http://' has to be in the article title 2) no print if 'http://' already included\n",
    "    if 'http://' not in i and 'video' not in i:\n",
    "        new = \"http://www.nbcnews.com\" + i\n",
    "        perfin_New.append(new)   \n",
    "final_perfin = perfin_New[0:5]\n",
    "# print(final_perfin)\n",
    "\n",
    "#For each article, prints all text paragraphs to a .txt file\n",
    "#Tokenizes each article for Date, PerFin, Tokenized Word, .txt\n",
    "for i in final_perfin:\n",
    "    htmler_perfin = urlopen(i)\n",
    "    souper_perfin = BeautifulSoup(htmler_perfin, \"lxml\")\n",
    "    file = souper_perfin.find('title').get_text()[0:20]\n",
    "    words = word_tokenize(file)\n",
    "    stop_title = str(date + \" PerFin \" + words[0] + \".txt\")\n",
    "#     print(stop_title)\n",
    "    f_perfin = open(stop_title, 'w')\n",
    "    for node in souper_perfin.findAll('p'):\n",
    "        f_perfin = open(stop_title, 'a')\n",
    "        out = str(''.join(node.findAll(text=True)))\n",
    "        if str(\"Live:\") or str(\"live:\") not in out:\n",
    "            f_perfin.write(out)\n",
    "        f_perfin.write(\"\\n\")\n",
    "    f_perfin.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brown', 'cat', 'love', 'black', 'food', 'eat', 'mouse', 'dog', 'neither']\n",
      "[0, 1, 0, 0, 1, 1, 0, 1, 0]\n",
      "[1, 2, 0, 1, 1, 1, 0, 0, 0]\n",
      "[0, 1, 1, 0, 0, 0, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "i = datetime.datetime.now()\n",
    "date = str(i.month) + \".\" + str(i.day) + \".\" + str(i.year)\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# os.chdir(\"Test\")\n",
    "total = []\n",
    "lists = []\n",
    "sentences = ['The cat and the dog eat their food.', \n",
    "            'Black cats and brown cats eat their food.', \n",
    "            'Neither cats nor dogs love mice.']\n",
    "\n",
    "\n",
    "# For every article in the Test File: 1) Change to Lowercase\n",
    "# 2) Remove Numbers 3) Tokenize/Remove Stop Words 4) POS Tagging\n",
    "# 5) Lemmatize using POS\n",
    "\n",
    "for i in sentences:\n",
    "#     filename = open(str(i), 'r')\n",
    "#     new = filename.read().lower()\n",
    "    ## Removes Numbers from File\n",
    "#     result = ''.join([i for i in sentences if not i.isdigit()])\n",
    "    ## Removes Punctuation from File\n",
    "    exclude = set(string.punctuation)\n",
    "    nopunct = ''.join(ch.lower() for ch in i if ch not in exclude)\n",
    "    ## Tokenize Article & Remove Stop Words\n",
    "    tok = tokenizer.tokenize(nopunct)\n",
    "    no_stop = []\n",
    "    for i in tok:\n",
    "        if i not in stop_words:\n",
    "            no_stop.append(str(i))\n",
    "    # print(no_stop)\n",
    "    ## Get POS tag for each word in list\n",
    "    ## Better for lemmatizing\n",
    "    pos = []\n",
    "    for j in no_stop:\n",
    "        p = nltk.tag.pos_tag([j])\n",
    "        pos.append(p)\n",
    "    # print(pos)\n",
    "    ## Lemmatizing List with POS Tag\n",
    "    lem = []\n",
    "    for k in pos:\n",
    "        w = k[0][0]\n",
    "        p = k[0][1]\n",
    "        if p.startswith('J'):\n",
    "            m = 'a'\n",
    "        elif p.startswith('V'):\n",
    "            m = 'v'\n",
    "        elif p.startswith('N'):\n",
    "            m = 'n'\n",
    "        elif p.startswith('R'):\n",
    "            m = 'r'\n",
    "        name = lemmatizer.lemmatize(str(w), pos = str(m))\n",
    "        lem.append(name)\n",
    "        sent = ' '.join(word for word in lem)\n",
    "    total.append(sent)\n",
    "# print(total)\n",
    "\n",
    "# Function for writing a Document Term Matrix for Modeling\n",
    "# Takes words that appear in 10% of articles\n",
    "# Writes matrix to txt file\n",
    "\n",
    "def termdocumentmatrix_example():\n",
    "    tdm = textmining.TermDocumentMatrix()\n",
    "    tdms = []\n",
    "    for line in total:\n",
    "        tdm.add_doc(line)\n",
    "        tdms.append(tdm)\n",
    "    myfile = open('featuresexample.txt', 'w')\n",
    "    write = csv.writer(myfile)\n",
    "    for row in tdm.rows(cutoff=70):\n",
    "        write.writerow(row)\n",
    "#         print(row)\n",
    "\n",
    "# termdocumentmatrix_example()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ours', 'not', 'have', 'at', 'above', 'off', 'didn', 'mightn', 'ma', 'more', 'having', 'they', 'then', 'during', 'the', 'them', 'and', 'own', 'too', 'few', 'himself', 'each', 'yours', 'which', 'from', 'were', 'below', 's', 'had', 'by', 'through', 'won', 'he', 'or', 'in', 'theirs', 'nor', 'aren', 'couldn', 'it', 'this', 'for', 'their', 'hasn', 'mustn', 'me', 'myself', 'with', 'some', 'an', 'because', 'into', 'down', 'any', 'been', 'when', 'where', 'doesn', 'needn', 'themselves', 'only', 'doing', 'wasn', 'but', 'don', 'very', 'until', 'has', 'over', 'yourself', 'all', 'does', 'whom', 'is', 'here', 'between', 'no', 'both', 'while', 'out', 'again', 'other', 'herself', 'your', 'those', 'do', 'will', 't', 'about', 'are', 'o', 'haven', 'that', 'who', 'being', 'under', 'd', 'before', 'her', 'our', 'a', 'as', 'ain', 'after', 'was', 'further', 'most', 're', 'so', 'now', 'isn', 'why', 'weren', 'if', 'you', 'there', 'i', 'wouldn', 'its', 'am', 'we', 'on', 'up', 'should', 'to', 'shouldn', 'against', 'y', 'itself', 'be', 'him', 'just', 've', 'ourselves', 'how', 'of', 'his', 'than', 'can', 'm', 'hadn', 'yourselves', 'my', 'll', 'such', 'she', 'same', 'once', 'hers', 'what', 'these', 'shan', 'did'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CurrentCat</th>\n",
       "      <th>rather</th>\n",
       "      <th>offer</th>\n",
       "      <th>realize</th>\n",
       "      <th>lot</th>\n",
       "      <th>office</th>\n",
       "      <th>whether</th>\n",
       "      <th>purchase</th>\n",
       "      <th>there</th>\n",
       "      <th>impact</th>\n",
       "      <th>...</th>\n",
       "      <th>course</th>\n",
       "      <th>next.</th>\n",
       "      <th>live</th>\n",
       "      <th>today</th>\n",
       "      <th>major</th>\n",
       "      <th>wont</th>\n",
       "      <th>hit</th>\n",
       "      <th>two</th>\n",
       "      <th>industrial</th>\n",
       "      <th>found</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>Low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>Medium</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>Low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>High</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>High</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>Low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>Medium</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>Low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>High</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>Low</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 466 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    CurrentCat  rather  offer  realize  lot  office  whether  purchase  there  \\\n",
       "419        Low       0      0        0    0       0        0         0      0   \n",
       "681     Medium       0      0        0    0       0        0         0      0   \n",
       "565        Low       0      0        0    0       0        0         1      0   \n",
       "207       High       0      0        0    0       0        0         0      0   \n",
       "528       High       0      0        0    0       0        0         0      0   \n",
       "562        Low       0      0        0    1       1        0         1      0   \n",
       "152     Medium       0      0        0    0       1        2         0      0   \n",
       "233        Low       0      0        1    0       0        1         0      1   \n",
       "253       High       0      0        1    0       0        1         0      1   \n",
       "340        Low       1      0        0    1       0        0         0      1   \n",
       "\n",
       "     impact  ...    course  next.  live  today  major  wont  hit  two  \\\n",
       "419       1  ...         0      1     0      0      1     0    2    0   \n",
       "681       0  ...         0      1     0      1      1     0    3    0   \n",
       "565       0  ...         0      1     0      0      1     0    0    2   \n",
       "207       0  ...         0      1     0      0      0     0    0    0   \n",
       "528       0  ...         0      0     0      0      0     0    2    0   \n",
       "562       1  ...         0      1     0      1      0     0    0    5   \n",
       "152       0  ...         0      0     0      0      0     0    0    2   \n",
       "233       0  ...         1      1     0      1      1     1    0    2   \n",
       "253       0  ...         1      1     0      1      1     1    0    2   \n",
       "340       0  ...         0      1     0      1      0     1    0    0   \n",
       "\n",
       "     industrial  found  \n",
       "419           1      0  \n",
       "681           1      0  \n",
       "565           0      0  \n",
       "207           1      0  \n",
       "528           0      0  \n",
       "562           0      0  \n",
       "152           0      3  \n",
       "233           0      0  \n",
       "253           0      0  \n",
       "340           0      2  \n",
       "\n",
       "[10 rows x 466 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "path = \"C:\\\\Users\\Andrew\\Google Drive\\School\\Mizzou\\Thesis\\Data and Code\"\n",
    "os.chdir(path)\n",
    "# os.getcwd()\n",
    "\n",
    "# Potentially use for Data Frame Manipulation\n",
    "# df_feat = pd.read_csv('features.txt')\n",
    "# df_feat.head(10)\n",
    "# df_resp = pd.read_csv('Volatility Numbers.xlsx')\n",
    "# df_resp.head(10)\n",
    "\n",
    "df = pd.read_csv('currentdata.csv')\n",
    "df = pd.DataFrame(df)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "df.head(10)\n",
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "'''\n",
    "input -> weights -> \n",
    "hidden layer 1 (activation function) -> \n",
    "hidden layer 2 (activation function) -> \n",
    "weights -> output layer\n",
    "\n",
    "compare output to intended output -> cost function (cross entropy)\n",
    "optimization function (optimizer) -> minimizes cost function (Adam, SGD, ...)\n",
    "^ manipulates weights (backpropogation)\n",
    "\n",
    "feed forward + backprop = epoch -> minimizes cost function each epoch\n",
    "'''\n",
    "\n",
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "# mnist = input_data.read_data_sets(\"/tmp/data\", one_hot=True)\n",
    "\n",
    "# 10 classes, 0-9\n",
    "'''\n",
    "0 = 0, 1 = 1, 2 = 2, ...\n",
    "one_hot ->\n",
    "0 = [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "1 = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "2 = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "'''\n",
    "\n",
    "n_nodes_hl1 = 500\n",
    "n_nodes_hl2 = 500\n",
    "n_nodes_hl3 = 500\n",
    "\n",
    "n_classes = 3\n",
    "# batches of 100 features to feed through network\n",
    "batch_size = 100\n",
    "\n",
    "x = tf.placeholder('float',[None, 465])\n",
    "y = tf.placeholder('float')\n",
    "\n",
    "def neural_network_model(data):\n",
    "    # (input_data * weights) + biases\n",
    "    #creates an array of data using random weights\n",
    "    hidden_1_layer = {'weights': tf.Variable(tf.random_normal([465, n_nodes_hl1])), \n",
    "                      'biases': tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
    "    \n",
    "    hidden_2_layer = {'weights': tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])), \n",
    "                      'biases': tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
    "    \n",
    "    hidden_3_layer = {'weights': tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])), \n",
    "                      'biases': tf.Variable(tf.random_normal([n_nodes_hl3]))}\n",
    "    \n",
    "    output_layer = {'weights': tf.Variable(tf.random_normal([n_nodes_hl3, n_classes])), \n",
    "                      'biases': tf.Variable(tf.random_normal([n_classes]))}\n",
    "    \n",
    "    l1 = tf.add(tf.matmul(data, hidden_1_layer['weights']), hidden_1_layer['biases'])\n",
    "    l1 = tf.nn.relu(l1)\n",
    "    \n",
    "    l2 = tf.add(tf.matmul(l1, hidden_2_layer['weights']), hidden_2_layer['biases'])\n",
    "    l2 = tf.nn.relu(l2)\n",
    "    \n",
    "    l3 = tf.add(tf.matmul(l2, hidden_3_layer['weights']), hidden_3_layer['biases'])\n",
    "    l3 = tf.nn.relu(l3)\n",
    "    \n",
    "    output = tf.matmul(l3, output_layer['weights']) + output_layer['biases']\n",
    "    return output\n",
    "    \n",
    "def train_neural_network(x):\n",
    "    prediction = neural_network_model(x)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(prediction, y))\n",
    "    # minimize the cost\n",
    "    # learning_rate = 0.001 default\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "    \n",
    "    hm_epochs = 10\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for epoch in range(hm_epochs):\n",
    "            epoch_loss = 0\n",
    "            for _ in range(int(df.train.num_examples/batch_size)):\n",
    "                epoch_x, epoch_y = df.train.next_batch(batch_size)\n",
    "                _, c = sess.run([optimizer, cost], feed_dict = {x: epoch_x, y: epoch_y})\n",
    "                epoch_loss += c\n",
    "            print('Epoch', epoch, 'completed out of', hm_epochs, 'loss:', epoch_loss)\n",
    "            \n",
    "        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "        print('Accuracy:' , accuracy.eval({x:df.test.images, y:df.test.labels}))\n",
    "\n",
    "train_neural_network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Mul_4:0\", shape=(), dtype=int32)\n",
      "30\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x1 = tf.constant(5)\n",
    "x2 = tf.constant(6)\n",
    "\n",
    "# Defined a model\n",
    "result = tf.mul(x1,x2)\n",
    "print(result)\n",
    "\n",
    "# No results until you define the model\n",
    "sess = tf.Session()\n",
    "print(sess.run(result))\n",
    "sess.close()\n",
    "\n",
    "# Better method (Don't have to close)\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(result))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
